{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d918030d-f3a2-4001-9f64-1a728a1e5fcc",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb048c59-bf1c-402f-a7cf-9c71a210ad13",
   "metadata": {},
   "source": [
    "Ans - Relationship between Polynomial Functions and Kernel Functions in Machine Learning Algorithms\n",
    "\n",
    "In machine learning, polynomial functions and kernel functions are closely related, especially in the context of Support Vector Machines (SVMs) and other kernelized models.\n",
    "\n",
    "Polynomial Functions\n",
    "\n",
    "Polynomial functions are mathematical expressions involving a sum of powers in one or more variables multiplied by coefficients. For example:\n",
    "\n",
    "f(x) = a + bx + cx^2 + dx^3\n",
    "\n",
    "In machine learning, polynomial functions can be used to model complex relationships between features. However, directly using high-degree polynomials can lead to overfitting, where the model learns the noise in the training data rather than the underlying patterns.\n",
    "\n",
    "Kernel Functions\n",
    "\n",
    "Kernel functions offer a solution to the overfitting problem associated with high-degree polynomials. A kernel function implicitly maps the input data into a higher-dimensional space without explicitly computing the transformation. This allows the model to learn complex, non-linear patterns without the computational cost of working with high-dimensional data.\n",
    "\n",
    "The Polynomial Kernel\n",
    "\n",
    "The polynomial kernel is a specific type of kernel function that is based on polynomial functions. It takes the following form:\n",
    "\n",
    "K(x, y) = (x · y + c)^d\n",
    "\n",
    "where:\n",
    "\n",
    "a. x and y are input feature vectors\n",
    "\n",
    "b. c is a constant term\n",
    "\n",
    "c. d is the degree of the polynomial\n",
    "\n",
    "The polynomial kernel implicitly computes the dot product of the input vectors raised to the power of d, along with a constant term. This transformation effectively maps the input data into a higher-dimensional space where it becomes easier to separate the classes using a linear hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c74ee-929c-47c9-bffe-029d96915e33",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31585c31-7dd1-4f7d-8fb6-3f5d35a17776",
   "metadata": {},
   "source": [
    "1] Import necessary modules:\n",
    "\n",
    "a. SVC: Class for Support Vector Classification\n",
    "\n",
    "b. train_test_split: Function to split data\n",
    "\n",
    "c. load_iris: Sample dataset\n",
    "\n",
    "d. accuracy_score: Metric for evaluation\n",
    "\n",
    "2] Load data:\n",
    "\n",
    "a. We load the Iris dataset (or you can use your own).\n",
    "\n",
    "b. X contains the features, and y contains the labels.\n",
    "\n",
    "3] Split data:\n",
    "\n",
    "a. We divide the data into training and testing sets.\n",
    "\n",
    "b. test_size=0.2 means 20% of data is used for testing.\n",
    "\n",
    "4] Create SVM:\n",
    "\n",
    "a. We create an SVC object, specifying:\n",
    "\n",
    "b. kernel='poly': Use the polynomial kernel.\n",
    "\n",
    "c. degree=3: The power of the polynomial (you can change this).\n",
    "\n",
    "d. C=1.0: Regularization parameter (balance between simplicity and accuracy).\n",
    "\n",
    "5] Train the SVM:\n",
    "\n",
    "a. svm.fit(X_train, y_train) learns the optimal model from the training data.\n",
    "\n",
    "6] Make predictions:\n",
    "\n",
    "a. svm.predict(X_test) applies the learned model to the test data.\n",
    "\n",
    "7] Evaluate accuracy:\n",
    "\n",
    "a. accuracy_score compares the predicted labels (y_pred) to the true labels (y_test) and calculates the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff75687f-0408-4474-a1c0-4b2328cead69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) \n",
    "\n",
    "svm = SVC(kernel='poly', degree=3, C=1.0)  \n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870db1b0-9213-49cf-b836-b73f75701d3f",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2154412-92cf-4608-b7c6-e374cc683a23",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), the epsilon (ε) parameter is crucial in determining the model's flexibility and error tolerance.  It defines the width of the \"epsilon-insensitive tube\" surrounding the predicted regression line. Any data points falling within this tube are considered to have zero error, while those outside contribute to the loss function.\n",
    "\n",
    "When epsilon is increased, the tube widens, allowing more data points to fall within it. Consequently, fewer points are classified as outliers or support vectors, leading to a simpler model with a higher bias and lower variance. This results in smoother predictions but may underfit the data if the underlying relationship is complex.\n",
    "\n",
    "Conversely, decreasing epsilon narrows the tube, making the model more sensitive to errors and increasing the number of support vectors. This leads to a more complex model with lower bias but higher variance, capturing intricate patterns in the data but potentially overfitting, especially in the presence of noise.\n",
    "\n",
    "Therefore, the choice of epsilon value involves a trade-off between model complexity and error tolerance. A larger epsilon prioritizes a simpler, smoother model that may not capture all nuances in the data, while a smaller epsilon favors a more complex model that could potentially overfit.\n",
    "\n",
    "Ultimately, the optimal epsilon value depends on the specific dataset and the desired level of accuracy and generalization. Cross-validation techniques can be employed to empirically determine the best epsilon value for a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f33c1-dee5-46da-a41b-fba127f38602",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdd3d8c-7c43-42dd-9e6a-1f4ede2b6ac7",
   "metadata": {},
   "source": [
    "1] Kernel Function\n",
    "\n",
    "Function: The kernel function transforms the input data into a higher-dimensional space where linear separation may be possible. This allows SVR to model non-linear relationships between the features and the target variable.\n",
    "\n",
    "Types: Common kernel functions include:\n",
    "\n",
    "Linear: Suitable for linearly separable data.\n",
    "\n",
    "Polynomial: Captures polynomial relationships of a specified degree.\n",
    "\n",
    "Radial Basis Function (RBF): Widely used, works well for many types of data, captures complex non-linear relationships.\n",
    "\n",
    "Sigmoid: Less common, similar to neural networks.\n",
    "\n",
    "Choice: The choice of kernel function depends on the nature of the data and the complexity of the relationship you expect.\n",
    "\n",
    "Increase Complexity: If you suspect a complex non-linear relationship, RBF or polynomial kernels (with higher degrees) are good choices.\n",
    "\n",
    "Decrease Complexity: If the relationship is relatively simple or linear, a linear kernel is often sufficient.\n",
    "\n",
    "2] C Parameter (Regularization)\n",
    "\n",
    "Function: Controls the trade-off between maximizing the margin (distance between the support vectors and the hyperplane) and minimizing the training error.\n",
    "\n",
    "Values:\n",
    "\n",
    "Large C: Prioritizes minimizing training error, potentially leading to overfitting.\n",
    "\n",
    "Small C: Prioritizes a wider margin, leading to a simpler model that may underfit.\n",
    "\n",
    "Choice:\n",
    "\n",
    "Increase C: If you have a lot of noisy data or want to prioritize capturing every data point.\n",
    "\n",
    "Decrease C: If you have a smaller dataset or want to prioritize generalization to new data.\n",
    "\n",
    "3] Epsilon Parameter (ε)\n",
    "\n",
    "Function: Defines the width of the \"epsilon-insensitive tube\" around the predicted values. Points within this tube are not penalized, while those outside contribute to the loss function.\n",
    "\n",
    "Values:\n",
    "\n",
    "Large ε: Allows more data points to fall within the tube, resulting in a simpler model.\n",
    "\n",
    "Small ε: Makes the model more sensitive to errors, potentially leading to a more complex model.\n",
    "\n",
    "Choice:\n",
    "\n",
    "Increase ε: If you want a smoother prediction or have noisy data.\n",
    "\n",
    "Decrease ε: If you want to capture finer details in the data or have a small amount of noise.\n",
    "\n",
    "4] Gamma Parameter (γ)\n",
    "\n",
    "Function:  Controls the influence of a single training example. It determines how far the influence of a single training example reaches.\n",
    "\n",
    "Values:\n",
    "\n",
    "Large γ: Makes the model more complex, potentially overfitting.\n",
    "\n",
    "Small γ: Makes the model simpler, potentially underfitting.\n",
    "\n",
    "Choice:\n",
    "\n",
    "Increase γ: If you have a lot of data points or the data is complex.\n",
    "\n",
    "Decrease γ: If you have a smaller dataset or the data is simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81327aa2-0d58-4c24-8109-79e9e62c9df9",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "\n",
    "1] Import the necessary libraries and load the dataseg\n",
    "\n",
    "2] Split the dataset into training and testing setZ\n",
    "\n",
    "3] Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "\n",
    "4] Create an instance of the SVC classifier and train it on the training datW\n",
    "\n",
    "5] hse the trained classifier to predict the labels of the testing datW\n",
    "\n",
    "6] Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "\n",
    "7] Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performanc_\n",
    "\n",
    "8] Train the tuned classifier on the entire dataseg\n",
    "\n",
    "9] Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c448c95-3705-47c8-8160-915095c0cb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        34\n",
      "           1       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        49\n",
      "   macro avg       1.00      1.00      1.00        49\n",
      "weighted avg       1.00      1.00      1.00        49\n",
      "\n",
      "Best parameters: {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "Tuned Model Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        34\n",
      "           1       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        49\n",
      "   macro avg       1.00      1.00      1.00        49\n",
      "weighted avg       1.00      1.00      1.00        49\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_svm_model.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('tips')\n",
    "label_encoder = LabelEncoder()\n",
    "df['time'] = label_encoder.fit_transform(df['time'])\n",
    "\n",
    "categorical_features = ['sex', 'smoker', 'day']\n",
    "ct = ColumnTransformer([('encoder', OneHotEncoder(), categorical_features)], remainder='passthrough')\n",
    "X = ct.fit_transform(df.drop(columns=['tip']))\n",
    "y = df['time']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "svm_model = SVC(probability=True)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_test_scaled)\n",
    "print(\"Initial Model Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(svm_model, param_grid, scoring='f1', cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "y_pred_tuned = best_svm.predict(X_test_scaled)\n",
    "print(\"Tuned Model Report:\")\n",
    "print(classification_report(y_test, y_pred_tuned))\n",
    "\n",
    "best_svm.fit(scaler.transform(X), y) \n",
    "import joblib\n",
    "joblib.dump(best_svm, 'best_svm_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33707829-85e8-4bc5-98a4-480f7e65f111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
